{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition (SER)\n",
    "\n",
    "This notebook builds a Speech Emotion Recognition model using 4 popular datasets:\n",
    "- RAVDESS\n",
    "- CREMA-D\n",
    "- TESS\n",
    "- SAVEE\n",
    "\n",
    "## Emotions Covered:\n",
    "- Angry\n",
    "- Disgust\n",
    "- Fear\n",
    "- Happy\n",
    "- Neutral\n",
    "- Sad\n",
    "- Surprise (in some datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "Python version: 3.14.0 (main, Oct  7 2025, 09:34:52) [Clang 17.0.0 (clang-1700.4.4.1)]\n",
      "Platform: Darwin arm64\n",
      "\n",
      "Installing kagglehub...\n",
      "✓ kagglehub installed\n",
      "Installing numpy...\n",
      "✓ numpy installed\n",
      "Installing pandas...\n",
      "✓ pandas installed\n",
      "Installing librosa...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import platform\n",
    "\n",
    "# List of required packages\n",
    "packages = [\n",
    "    'kagglehub',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'librosa',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'scikit-learn',\n",
    "    'soundfile'  # Required by librosa for audio file handling\n",
    "]\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.system()} {platform.machine()}\\n\")\n",
    "\n",
    "# Install common packages\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "        print(f\"✓ {package} installed\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"✗ Failed to install {package}: {e}\")\n",
    "\n",
    "# Install TensorFlow based on platform\n",
    "print(\"\\nInstalling TensorFlow...\")\n",
    "try:\n",
    "    if platform.system() == 'Darwin' and platform.machine() == 'arm64':\n",
    "        # macOS with Apple Silicon\n",
    "        print(\"Detected macOS with Apple Silicon\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'tensorflow-macos'])\n",
    "        print(\"✓ tensorflow-macos installed\")\n",
    "    else:\n",
    "        # Other platforms\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'tensorflow'])\n",
    "        print(\"✓ tensorflow installed\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"✗ TensorFlow installation failed\")\n",
    "    print(\"⚠ TensorFlow may not support your Python version yet\")\n",
    "    print(f\"  Current Python: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "    print(\"  Try using Python 3.9, 3.10, 3.11, or 3.12\")\n",
    "    print(\"\\nAlternatives:\")\n",
    "    print(\"1. Create a new virtual environment with Python 3.11:\")\n",
    "    print(\"   python3.11 -m venv .venv\")\n",
    "    print(\"   source .venv/bin/activate\")\n",
    "    print(\"2. Use conda: conda create -n ser python=3.11 tensorflow\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ismatsamadov/speech_emotion_recognition/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/dmitrybabko/speech-emotion-recognition-en?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987M/987M [17:26<00:00, 989kB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/ismatsamadov/.cache/kagglehub/datasets/dmitrybabko/speech-emotion-recognition-en/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"dmitrybabko/speech-emotion-recognition-en\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.util._print_versions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlibrosa\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlibrosa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/speech_emotion_recognition/.venv/lib/python3.14/site-packages/pandas/__init__.py:140\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_print_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# excel\u001b[39;00m\n\u001b[32m    144\u001b[39m     ExcelFile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     read_spss,\n\u001b[32m    173\u001b[39m )\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_normalize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m json_normalize\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas.util._print_versions'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Librosa version:\", librosa.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the dataset directory\n",
    "import glob\n",
    "\n",
    "# Update this path based on where kagglehub downloaded the data\n",
    "data_path = path\n",
    "\n",
    "# Find all subdirectories\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    print(f\"Directory: {root}\")\n",
    "    print(f\"Subdirectories: {dirs}\")\n",
    "    print(f\"Number of files: {len(files)}\")\n",
    "    if len(files) > 0 and files[0].endswith('.wav'):\n",
    "        print(f\"Sample files: {files[:3]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Label Extraction\n",
    "\n",
    "We'll create functions to extract emotion labels from each dataset based on their naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_ravdess(filename):\n",
    "    \"\"\"\n",
    "    Extract emotion from RAVDESS filename\n",
    "    Format: 03-01-{emotion}-01-01-01-01.wav\n",
    "    Emotions: 01=neutral, 02=calm, 03=happy, 04=sad, 05=angry, 06=fearful, 07=disgust, 08=surprised\n",
    "    \"\"\"\n",
    "    emotion_map = {\n",
    "        '01': 'neutral',\n",
    "        '02': 'calm',\n",
    "        '03': 'happy',\n",
    "        '04': 'sad',\n",
    "        '05': 'angry',\n",
    "        '06': 'fear',\n",
    "        '07': 'disgust',\n",
    "        '08': 'surprise'\n",
    "    }\n",
    "    parts = os.path.basename(filename).split('-')\n",
    "    if len(parts) >= 3:\n",
    "        return emotion_map.get(parts[2], 'unknown')\n",
    "    return 'unknown'\n",
    "\n",
    "def get_emotion_crema(filename):\n",
    "    \"\"\"\n",
    "    Extract emotion from CREMA-D filename\n",
    "    Format: 1001_DFA_ANG_XX.wav\n",
    "    Emotions: SAD, ANG, DIS, FEA, HAP, NEU\n",
    "    \"\"\"\n",
    "    emotion_map = {\n",
    "        'SAD': 'sad',\n",
    "        'ANG': 'angry',\n",
    "        'DIS': 'disgust',\n",
    "        'FEA': 'fear',\n",
    "        'HAP': 'happy',\n",
    "        'NEU': 'neutral'\n",
    "    }\n",
    "    parts = os.path.basename(filename).split('_')\n",
    "    if len(parts) >= 3:\n",
    "        return emotion_map.get(parts[2], 'unknown')\n",
    "    return 'unknown'\n",
    "\n",
    "def get_emotion_tess(filename):\n",
    "    \"\"\"\n",
    "    Extract emotion from TESS filename\n",
    "    Format: YAF_dog_angry.wav\n",
    "    Emotions are in the filename\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filename).lower()\n",
    "    if 'angry' in basename or 'anger' in basename:\n",
    "        return 'angry'\n",
    "    elif 'disgust' in basename:\n",
    "        return 'disgust'\n",
    "    elif 'fear' in basename:\n",
    "        return 'fear'\n",
    "    elif 'happy' in basename or 'happiness' in basename:\n",
    "        return 'happy'\n",
    "    elif 'neutral' in basename:\n",
    "        return 'neutral'\n",
    "    elif 'sad' in basename or 'sadness' in basename:\n",
    "        return 'sad'\n",
    "    elif 'surprise' in basename or 'surprised' in basename:\n",
    "        return 'surprise'\n",
    "    elif 'pleasant' in basename:\n",
    "        return 'happy'\n",
    "    return 'unknown'\n",
    "\n",
    "def get_emotion_savee(filename):\n",
    "    \"\"\"\n",
    "    Extract emotion from SAVEE filename\n",
    "    Format: a01.wav, d01.wav, etc.\n",
    "    a=anger, d=disgust, f=fear, h=happiness, n=neutral, sa=sadness, su=surprise\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filename).lower()\n",
    "    if basename.startswith('a'):\n",
    "        return 'angry'\n",
    "    elif basename.startswith('d'):\n",
    "        return 'disgust'\n",
    "    elif basename.startswith('f'):\n",
    "        return 'fear'\n",
    "    elif basename.startswith('h'):\n",
    "        return 'happy'\n",
    "    elif basename.startswith('n'):\n",
    "        return 'neutral'\n",
    "    elif basename.startswith('sa'):\n",
    "        return 'sad'\n",
    "    elif basename.startswith('su'):\n",
    "        return 'surprise'\n",
    "    return 'unknown'\n",
    "\n",
    "def get_emotion(filepath):\n",
    "    \"\"\"\n",
    "    Automatically detect dataset type and extract emotion\n",
    "    \"\"\"\n",
    "    filepath_lower = filepath.lower()\n",
    "    \n",
    "    if 'ravdess' in filepath_lower or 'actor_' in filepath_lower:\n",
    "        return get_emotion_ravdess(filepath)\n",
    "    elif 'crema' in filepath_lower:\n",
    "        return get_emotion_crema(filepath)\n",
    "    elif 'tess' in filepath_lower:\n",
    "        return get_emotion_tess(filepath)\n",
    "    elif 'savee' in filepath_lower:\n",
    "        return get_emotion_savee(filepath)\n",
    "    else:\n",
    "        # Try to infer from filename\n",
    "        if '-' in os.path.basename(filepath) and len(os.path.basename(filepath).split('-')) > 5:\n",
    "            return get_emotion_ravdess(filepath)\n",
    "        elif '_' in os.path.basename(filepath):\n",
    "            # Try CREMA first, then TESS\n",
    "            emotion = get_emotion_crema(filepath)\n",
    "            if emotion == 'unknown':\n",
    "                emotion = get_emotion_tess(filepath)\n",
    "            return emotion\n",
    "        else:\n",
    "            return get_emotion_savee(filepath)\n",
    "    \n",
    "# Test the functions\n",
    "print(\"Testing emotion extraction:\")\n",
    "print(\"RAVDESS test:\", get_emotion_ravdess(\"03-01-05-01-01-01-01.wav\"))\n",
    "print(\"CREMA test:\", get_emotion_crema(\"1001_DFA_ANG_XX.wav\"))\n",
    "print(\"TESS test:\", get_emotion_tess(\"YAF_dog_angry.wav\"))\n",
    "print(\"SAVEE test:\", get_emotion_savee(\"a01.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "\n",
    "We'll extract multiple audio features:\n",
    "- MFCC (Mel-frequency cepstral coefficients)\n",
    "- Chroma\n",
    "- Mel Spectrogram\n",
    "- Zero Crossing Rate\n",
    "- Spectral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path, duration=3, sr=22050):\n",
    "    \"\"\"\n",
    "    Extract comprehensive audio features from audio file\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: path to audio file\n",
    "    - duration: maximum duration to load (in seconds)\n",
    "    - sr: sampling rate\n",
    "    \n",
    "    Returns:\n",
    "    - features: numpy array of concatenated features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio, sample_rate = librosa.load(file_path, duration=duration, sr=sr)\n",
    "        \n",
    "        # Extract MFCC features (40 coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "        \n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "        chroma_mean = np.mean(chroma.T, axis=0)\n",
    "        \n",
    "        # Extract Mel Spectrogram\n",
    "        mel = librosa.feature.melspectrogram(y=audio, sr=sample_rate)\n",
    "        mel_mean = np.mean(mel.T, axis=0)\n",
    "        \n",
    "        # Extract Spectral Contrast\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate)\n",
    "        contrast_mean = np.mean(contrast.T, axis=0)\n",
    "        \n",
    "        # Extract Tonnetz (Tonal Centroid Features)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=sample_rate)\n",
    "        tonnetz_mean = np.mean(tonnetz.T, axis=0)\n",
    "        \n",
    "        # Extract Zero Crossing Rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio)\n",
    "        zcr_mean = np.mean(zcr)\n",
    "        \n",
    "        # Extract Spectral Rolloff\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate)\n",
    "        rolloff_mean = np.mean(spectral_rolloff)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        features = np.hstack([\n",
    "            mfccs_mean,\n",
    "            chroma_mean,\n",
    "            mel_mean,\n",
    "            contrast_mean,\n",
    "            tonnetz_mean,\n",
    "            zcr_mean,\n",
    "            rolloff_mean\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Feature extraction function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load All Audio Files and Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all .wav files from the dataset\n",
    "audio_files = []\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            audio_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Total audio files found: {len(audio_files)}\")\n",
    "\n",
    "# Display sample file paths\n",
    "if len(audio_files) > 0:\n",
    "    print(\"\\nSample file paths:\")\n",
    "    for i in range(min(5, len(audio_files))):\n",
    "        print(f\"{i+1}. {audio_files[i]}\")\n",
    "        print(f\"   Detected emotion: {get_emotion(audio_files[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels for all audio files\n",
    "print(\"Extracting features from all audio files...\")\n",
    "print(\"This may take several minutes depending on the dataset size.\\n\")\n",
    "\n",
    "features_list = []\n",
    "labels_list = []\n",
    "file_paths = []\n",
    "\n",
    "# Process files with progress indication\n",
    "total_files = len(audio_files)\n",
    "for idx, file_path in enumerate(audio_files, 1):\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processing: {idx}/{total_files} files ({idx/total_files*100:.1f}%)\")\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(file_path)\n",
    "    \n",
    "    if features is not None:\n",
    "        # Get emotion label\n",
    "        emotion = get_emotion(file_path)\n",
    "        \n",
    "        if emotion != 'unknown':\n",
    "            features_list.append(features)\n",
    "            labels_list.append(emotion)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "print(f\"\\nFeature extraction complete!\")\n",
    "print(f\"Successfully processed: {len(features_list)} files\")\n",
    "print(f\"Skipped: {total_files - len(features_list)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for easier analysis\n",
    "df = pd.DataFrame({\n",
    "    'file_path': file_paths,\n",
    "    'emotion': labels_list\n",
    "})\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nEmotion distribution:\")\n",
    "print(df['emotion'].value_counts())\n",
    "\n",
    "# Visualize emotion distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df, x='emotion', order=df['emotion'].value_counts().index)\n",
    "plt.title('Emotion Distribution in Dataset', fontsize=16)\n",
    "plt.xlabel('Emotion', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\nFeature vector shape: {features_list[0].shape}\")\n",
    "print(f\"Number of features per sample: {len(features_list[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to numpy arrays\n",
    "X = np.array(features_list)\n",
    "y = np.array(labels_list)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "print(f\"\\nEncoded labels shape: {y_categorical.shape}\")\n",
    "print(f\"Number of emotion classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"Emotion classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_categorical, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nData standardization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a deep neural network for speech emotion recognition\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Dense(512, activation='relu', input_shape=(input_shape,)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Hidden layers\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "num_features = X_train_scaled.shape[1]\n",
    "num_classes = y_categorical.shape[1]\n",
    "\n",
    "model = create_model(num_features, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_ser_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[0].set_title('Model Accuracy', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "axes[1].plot(history.history['loss'], label='Train Loss')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[1].set_title('Model Loss', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(\n",
    "    y_test_classes,\n",
    "    y_pred_classes,\n",
    "    target_names=label_encoder.classes_\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_\n",
    ")\n",
    "plt.title('Confusion Matrix - Speech Emotion Recognition', fontsize=16)\n",
    "plt.xlabel('Predicted Emotion', fontsize=12)\n",
    "plt.ylabel('True Emotion', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"=\"*40)\n",
    "for i, emotion in enumerate(label_encoder.classes_):\n",
    "    class_accuracy = cm[i, i] / cm[i].sum() * 100\n",
    "    print(f\"{emotion.capitalize()}: {class_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Model and Preprocessing Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the final model\n",
    "model.save('speech_emotion_recognition_model.keras')\n",
    "print(\"Model saved as 'speech_emotion_recognition_model.keras'\")\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Scaler saved as 'scaler.pkl'\")\n",
    "\n",
    "# Save the label encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"Label encoder saved as 'label_encoder.pkl'\")\n",
    "\n",
    "print(\"\\nAll model artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Prediction Function for New Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(audio_file_path, model, scaler, label_encoder):\n",
    "    \"\"\"\n",
    "    Predict emotion from a new audio file\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_file_path: path to the audio file\n",
    "    - model: trained Keras model\n",
    "    - scaler: fitted StandardScaler\n",
    "    - label_encoder: fitted LabelEncoder\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_emotion: predicted emotion label\n",
    "    - probabilities: dictionary of emotion probabilities\n",
    "    \"\"\"\n",
    "    # Extract features\n",
    "    features = extract_features(audio_file_path)\n",
    "    \n",
    "    if features is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Reshape and scale features\n",
    "    features_scaled = scaler.transform(features.reshape(1, -1))\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(features_scaled, verbose=0)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    predicted_emotion = label_encoder.classes_[predicted_class]\n",
    "    \n",
    "    # Get probabilities for all emotions\n",
    "    probabilities = {}\n",
    "    for i, emotion in enumerate(label_encoder.classes_):\n",
    "        probabilities[emotion] = prediction[0][i] * 100\n",
    "    \n",
    "    return predicted_emotion, probabilities\n",
    "\n",
    "# Test prediction on a random test sample\n",
    "if len(file_paths) > 0:\n",
    "    test_file = file_paths[0]\n",
    "    print(f\"Testing prediction on: {os.path.basename(test_file)}\")\n",
    "    print(f\"Actual emotion: {get_emotion(test_file)}\")\n",
    "    \n",
    "    predicted_emotion, probabilities = predict_emotion(test_file, model, scaler, label_encoder)\n",
    "    \n",
    "    if predicted_emotion:\n",
    "        print(f\"\\nPredicted emotion: {predicted_emotion}\")\n",
    "        print(\"\\nProbabilities for all emotions:\")\n",
    "        for emotion, prob in sorted(probabilities.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {emotion.capitalize()}: {prob:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Visualize Sample Audio and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio_and_prediction(audio_file_path, model, scaler, label_encoder):\n",
    "    \"\"\"\n",
    "    Visualize audio waveform, spectrogram, and emotion prediction\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(audio_file_path, duration=3)\n",
    "    \n",
    "    # Get prediction\n",
    "    predicted_emotion, probabilities = predict_emotion(\n",
    "        audio_file_path, model, scaler, label_encoder\n",
    "    )\n",
    "    actual_emotion = get_emotion(audio_file_path)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Waveform\n",
    "    axes[0, 0].set_title('Waveform', fontsize=12)\n",
    "    librosa.display.waveshow(y, sr=sr, ax=axes[0, 0])\n",
    "    axes[0, 0].set_xlabel('Time')\n",
    "    axes[0, 0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Spectrogram', fontsize=12)\n",
    "    fig.colorbar(img, ax=axes[0, 1], format='%+2.0f dB')\n",
    "    \n",
    "    # MFCC\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    img2 = librosa.display.specshow(mfccs, sr=sr, x_axis='time', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('MFCC', fontsize=12)\n",
    "    fig.colorbar(img2, ax=axes[1, 0])\n",
    "    \n",
    "    # Prediction probabilities\n",
    "    emotions = list(probabilities.keys())\n",
    "    probs = list(probabilities.values())\n",
    "    colors = ['green' if e == predicted_emotion else 'blue' for e in emotions]\n",
    "    \n",
    "    axes[1, 1].barh(emotions, probs, color=colors, alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Probability (%)', fontsize=10)\n",
    "    axes[1, 1].set_title(\n",
    "        f'Predicted: {predicted_emotion} | Actual: {actual_emotion}',\n",
    "        fontsize=12,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.suptitle(\n",
    "        f'Audio Analysis: {os.path.basename(audio_file_path)}',\n",
    "        fontsize=14,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a sample\n",
    "if len(file_paths) > 0:\n",
    "    sample_file = file_paths[10] if len(file_paths) > 10 else file_paths[0]\n",
    "    visualize_audio_and_prediction(sample_file, model, scaler, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "\n",
    "1. Downloaded the Speech Emotion Recognition dataset from Kaggle\n",
    "2. Processed 4 different datasets (RAVDESS, CREMA-D, TESS, SAVEE)\n",
    "3. Extracted comprehensive audio features (MFCC, Chroma, Mel Spectrogram, etc.)\n",
    "4. Built a deep neural network for emotion classification\n",
    "5. Trained the model with proper validation and callbacks\n",
    "6. Evaluated the model performance\n",
    "7. Created visualization tools for predictions\n",
    "8. Saved the trained model for future use\n",
    "\n",
    "The model can now predict emotions from speech audio files!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
